%% \VignetteIndexEntry{Neighbourhood functions}
\documentclass[a4paper,11pt]{article}
\usepackage[left = 3cm, top = 2cm, bottom = 2cm, right = 4cm, marginparwidth=3.5cm]{geometry}
\usepackage[noae,nogin]{Sweave}
\usepackage{libertine}
\usepackage[scaled=0.9]{inconsolata}
\usepackage[T1]{fontenc}
\renewcommand*\familydefault{\sfdefault}
\usepackage{amsmath,amstext}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{framed}
\usepackage{ragged2e}
\usepackage[hang]{footmisc}
\definecolor{grau2}{rgb}{.2,.2,.2}
\definecolor{grau7}{rgb}{.7,.7,.7}
\DefineVerbatimEnvironment{Sinput}{Verbatim}{}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{frame=single,
  xleftmargin=0em, formatcom=\color{grau2},rulecolor=\color{grau7}}
\DefineVerbatimEnvironment{Scode}{Verbatim}{xleftmargin=2em}
\fvset{listparameters={\setlength{\topsep}{0pt}}}
\renewenvironment{Schunk}{\vspace{\topsep}}{\vspace{\topsep}}
\SweaveOpts{keep.source = TRUE, eps = TRUE}

<<echo=false>>=
options(continue = "  ", digits = 3, width = 60, useFancyQuotes = FALSE)
pv <- packageVersion("neighbours")
pv <- gsub("(.*)[.](.*)", "\\1-\\2", pv)
@

\begin{document}
\title{}
\author{Enrico Schumann\\\url{es@enricoschumann.net}}

{\raggedright{\LARGE Neighbourhood Functions for Local-Search Algorithms\par}}\hspace*{\fill}
{\footnotesize Package version \Sexpr{pv}}\medskip

\noindent Enrico Schumann\\
\noindent \url{es@enricoschumann.net}\\
\bigskip

\noindent Local-search algorithms move through the
solution space by randomly selecting new solutions that
are close to the original solutions. Such a new
solution is called a neighbour solution.  The function
\texttt{neighbourfun} constructs a neighbourhood
function, i.e. a function that maps a given solution to a
randomly-chosen neighbour.


<<package-seed>>=
library("neighbours")
set.seed(34734)
@

\noindent In the examples that follow, we will use a
simple optimisation algorithm, a (stochastic) Local
Search.  If package \texttt{NMOF} is available,
function \texttt{LSopt} from that package is used;
otherwise, we use a simple replacement (taken from
\citealp[Chapter~13]{Gilli2019}).

<<LSopt>>=
LSopt <- if (requireNamespace("NMOF")) {
             NMOF::LSopt
         } else
             function(OF, algo = list(), ...) {
                 xc  <- algo$x0
                 xcF <- OF(xc, ...)
                 for (s in seq_len(algo$nS)) {
                     xn <- algo$neighbour(xc, ...)
                     xnF <- OF(xn, ...)
                     if (xnF <= xcF) {
                         xc  <- xn
                         xcF <- xnF
                     }
                 }
                 list(xbest = xc, OFvalue = xcF)
             }
@

\section*{Example: Selecting elements of a list}

We are given a numeric vector~$y$, and also a
matrix~$X$, which has as many rows as~$y$ has elements.
The aim now is to find a subset of columns of~$X$ whose
average is as lowly correlated with $y$ as possible.
Let us create random data.

<<data>>=
ny <- 50
nx <- 2000
y <- runif(ny)
X <- array(runif(ny * nx), dim = c(ny, nx))
@

We'll try a (stochastic) Local Search to compute a
solution.  There may be other, perhaps better
heuristics for the job. But a Local Search will compute
a good solution (as we will see), and it is simple,
which is a good idea for an example.  For a tutorial on
Local Search; see \citet[Chapter~13]{Gilli2019}.


Suppose we want a solution to include between 10 and
20~columns.  A possible solution would be this one.
<<x0>>=
x0 <- logical(nx)
x0[1:15] <- TRUE
head(x0, 20)
@

\noindent We write an objective function to compute the actual
quality of the solution \texttt{x0}.
<<objective-fun>>=
column_cor <- function(x, X, y)
    cor(rowMeans(X[, x]), y)
@

\noindent We evaluate the quality of~\texttt{x0}.
<<>>=
column_cor(x0, X, y)
@

\noindent To run a Local Search, we need a
neighbourhood function.  Calling \texttt{neighbourfun}
will create such a function, and it will bind the
parameters to this function.%
\marginpar{\footnotesize\RaggedRight
  \citet{Gentleman2000} show that \textsf{R}'s scoping
  rules are particularly convenient for
  optimisation.} %


<<nb>>=
nb <- neighbourfun(type = "logical", kmin = 10, kmax = 20)
@

\noindent It remains to run the Local Search.

<<LSopt-run>>=
sol.ls <- LSopt(column_cor,
                list(x0 = x0,
                     neighbour = nb,
                     nI = 1000,
                     printBar = FALSE),
                X = X, y = y)
@

Let us evaluate the final solution.
<<>>=
column_cor(sol.ls$xbest, X, y)
@

We may also visualise the initial and the final
solution.

<<fig=true, width = 5, height = 3.2>>=
par(mfrow = c(1, 2), las = 1, bty = "n",
    mar = c(3, 3, 1, 0.5), tck = 0.02, cex = 0.7,
    mgp = c(1.75, 0.25, 0))
plot(y, rowMeans(X[, x0]),
     main = "Initial solution",
     pch = 19, cex = 0.5,
     ylim = c(0.3, 0.7),
     ylab = "Linear combination of columns")
par(yaxt = "n")
plot(y, rowMeans(X[, sol.ls$xbest]),
     main = "Result of local search",
     pch = 19, cex = 0.5,
     ylim = c(0.2, 0.8),
     ylab = "Linear combination of columns")
axis(4)
@


\section*{More restrictions}

The neighbourhood function we used in the previous
section included constraints: it would include no fewer
than 10 or no more than 20~\texttt{TRUE} values. Note
that the neighbourhood function required a valid
\texttt{x} as input.%
\marginpar{\footnotesize\RaggedRight For invalid
  \texttt{x}, the result is undefined.  Neighbourhood
  functions should not check the validity of their
  inputs, because of speed: the functions are called
  thousands of times during an optimisation run, and
  every fraction of second matters.} %

We may also set \texttt{kmin} and \texttt{kmax} to
the same integer, so the number of \texttt{TRUE} values
is fixed.  (In this case, a slightly-different
neighbourhood algorithm will be used.)

We can also add a constraint about elements not to
touch. Suppose the initial solution is the following:
<<>>=
x <- logical(9)
x[4:6] <- TRUE
compare_vectors(x)
@

\noindent We restrict the changes that can be made to
the solution: the first three elements must not be
touched; only the remaining elements may change. (They
are \texttt{active}.)

<<restrict>>=
active <- !logical(length(x))
active[1:3] <- FALSE
active
nb <- neighbourfun(type = "logical", kmin = 3, kmax = 3,
                   active = active)
@

\noindent Again, let us take a few random steps.  The
element in the middle remains \texttt{TRUE}, just as
the first and last element remain \texttt{FALSE}.

<<echo=false>>=
xs <- list()
xs[[1]] <- x
for (i in 1:15) {
    xs[[length(xs) + 1]] <- x <- nb(x)
}
do.call(compare_vectors, xs)
@



\section*{Another example: minimising portfolio risk}

When the argument \texttt{type} is set to
\texttt{numeric}, the resulting neighbour function will
randomly select elements, and then add and subtract
typically-small numbers to and from those elements.
The size of those numbers is controlled by argument
\texttt{stepsize}, and with argument \texttt{random}
set to TRUE (the default), the step sizes will vary
randomly.



Suppose we are given a matrix \texttt{R} and aim to
find a vector $x$ such that the variance of the
elements in~$Rx$ is minimised.  This is common problem
in finance, in which $R$ could be a matrix of
asset-price returns, with each column holding the
returns of one asset, and $x$ a vector of portfolio
weights.

For this particular goal -- the variance of returns --,
we can first compute the variance--covariance
matrix~$\Sigma$ of $R$, and then minimise $x' \Sigma x$.

<<>>=
R <- NMOF::randomReturns(20, 120, 0.03, rho = 0.6)
S <- cov(R)  ## Sigma
@

Suppose we start with an equal-weight portfolio.

<<>>=
x0 <- rep(1/20, 20)
@ 

The objective function.
<<>>=
variance <- function(x, S, ...)
    x %*% S %*% x
variance(x0, S)
@ 

We can solve this problem with Local Search.
<<mv-ls1>>=
nb <- neighbourfun(type = "numeric", min = 0, max = 0.10,
                   stepsize = 0.005)
sol.ls <- LSopt(variance,
                list(x0 = x0,
                     neighbour = nb,
                     nI = 1000,
                     printBar = FALSE),
                S = S)

NMOF::minvar(S, wmin = 0, wmax = 0.1)
@

An alternative, and perhaps more-straightforward way to
write the objective function is the following.
<<>>=
variance2 <- function(x, R, ...)
    var(R %*% x)
@ 

When we feed it to \texttt{LSopt}, we arrive at the same solution.
<<mv-ls2>>=
nb <- neighbourfun(type = "numeric", min = 0, max = 0.10,
                   stepsize = 0.005)
sol.ls <- LSopt(variance2,
                list(x0 = x0,
                     neighbour = nb,
                     nI = 1000,
                     printBar = FALSE),
                R = R)

NMOF::minvar(S, wmin = 0, wmax = 0.1)
@

This second objective function is less
efficient than the first.  But it is much more flexible:
only for minimising variance,  could we take the shortcut via
the variance--covariance matrix.  But for other
measures of risk, we cannot do that.  One example is the so-called
semi-variance, defined as

\begin{equation}
  \label{eq:sv}
  \frac{1}{N}\sum_{i=1}^N \min(R_ix - m, 0)^2
\end{equation}

All we have to do now is to exchange objective functions.
<<>>=
semivariance <- function(x, R, ...) {
    Rx <- R %*% x
    Rx.lower <- pmin(Rx - mean(Rx), 0)
    sum(Rx.lower)
} 
@ 


\section*{Updating}


In many applications, such as the example in the
previous section, but also in many other cases when
doing data analysis, the solution \texttt{x} is used to
compute a matrix/vector product $Ax$, in which $A$ is a
$m$ times $n$~matrix, and $x$ is of length$n$.


If we only change few elements in the solution, we can
actually update this product and save computing time
(the longer $x$ is, the more time we can save).


Let $x^{\mathrm{c}}$ denote the current solution and
$x^{\mathrm{n}}$ the neighbour solution. This latter
solution is produced by adding element-wise the
sparse vector $x^{\Delta}$ to $x^{\mathrm{c}}$.

\begin{equation*}
   x^{\mathrm{n}}  = \phantom{A(}x^{\mathrm{c}} + x^{\Delta}
\end{equation*}

\begin{equation*}
  Ax^{\mathrm{n}}  = A(x^{\mathrm{c}}+ x^{\Delta}) =
  \underbrace{\ \ Ax^{\mathrm{c}}\ \ }_{\text{known}} + Ax^{\Delta}
\end{equation*}




<<num-target>>=
target <- runif(100)



deviation <- function(x, target) {
    xx <- x - target
    crossprod(xx)
}

sol <- LSopt(deviation,
             list(neighbour = neighbourfun(type = "numeric",
                                           length = 100,
                                           stepsize = 0.05),
                  x0 = runif(length(target)),
                  nI = 50000,
                  printBar = FALSE),
             target = target)

data.frame(target[1:10], sol$xbest[1:10])


@



\citet{Gilli2019}

\bibliographystyle{plainnat}
\bibliography{neighbours}

\end{document}
